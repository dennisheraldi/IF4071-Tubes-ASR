2023-12-01 17:28:30,607 - speechbrain.core - INFO - Beginning experiment!
2023-12-01 17:28:30,607 - speechbrain.core - INFO - Experiment folder: ./save
2023-12-01 17:28:30,857 - speechbrain.utils.superpowers - DEBUG - certifi==2023.11.17
charset-normalizer==3.3.2
filelock==3.13.1
fsspec==2023.10.0
huggingface-hub==0.19.4
HyperPyYAML==1.2.2
idna==3.6
Jinja2==3.1.2
joblib==1.3.2
MarkupSafe==2.1.3
mpmath==1.3.0
networkx==3.2.1
numpy==1.26.2
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu12==2.18.1
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu12==12.1.105
packaging==23.2
PyYAML==6.0.1
requests==2.31.0
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
scipy==1.11.4
sentencepiece==0.1.99
-e git+https://github.com/speechbrain/speechbrain/@3fcbbbabc020b8917f607566121d5c5c415769b4#egg=speechbrain
sympy==1.12
torch==2.1.1
torchaudio==2.1.1
tqdm==4.66.1
triton==2.1.0
typing_extensions==4.8.0
urllib3==2.1.0


2023-12-01 17:28:30,861 - speechbrain.utils.superpowers - DEBUG - 3fcbbbabc


2023-12-01 17:28:30,862 - speechbrain.tokenizers.SentencePiece - INFO - Train tokenizer with type:unigram
2023-12-01 17:28:30,862 - speechbrain.tokenizers.SentencePiece - INFO - Extract words sequences from:../train.json
2023-12-01 17:28:30,864 - speechbrain.tokenizers.SentencePiece - INFO - Text file created at: ./save/train.txt
2023-12-01 17:28:30,884 - speechbrain.tokenizers.SentencePiece - INFO - ==== Loading Tokenizer ===
2023-12-01 17:28:30,884 - speechbrain.tokenizers.SentencePiece - INFO - Tokenizer path: ./save/814_unigram.model
2023-12-01 17:28:30,884 - speechbrain.tokenizers.SentencePiece - INFO - Tokenizer vocab_size: 814
2023-12-01 17:28:30,884 - speechbrain.tokenizers.SentencePiece - INFO - Tokenizer type: unigram
2023-12-01 17:28:30,885 - speechbrain.tokenizers.SentencePiece - INFO - ==== Accuracy checking for recovering text from tokenizer ===
2023-12-01 17:28:30,908 - speechbrain.tokenizers.SentencePiece - INFO - recover words from: ../train.json
2023-12-01 17:28:30,909 - speechbrain.tokenizers.SentencePiece - WARNING - Wrong recover words: 1
2023-12-01 17:28:30,909 - speechbrain.tokenizers.SentencePiece - WARNING - Tokenizer vocab size: 814
2023-12-01 17:28:30,909 - speechbrain.tokenizers.SentencePiece - WARNING - accuracy recovering words: 0.9987714987714987
2023-12-01 17:28:30,909 - speechbrain.tokenizers.SentencePiece - INFO - ==== Accuracy checking for recovering text from tokenizer ===
2023-12-01 17:28:30,932 - speechbrain.tokenizers.SentencePiece - INFO - recover words from: ../valid.json
2023-12-01 17:28:30,933 - speechbrain.tokenizers.SentencePiece - WARNING - Wrong recover words: 1
2023-12-01 17:28:30,933 - speechbrain.tokenizers.SentencePiece - WARNING - Tokenizer vocab size: 814
2023-12-01 17:28:30,933 - speechbrain.tokenizers.SentencePiece - WARNING - accuracy recovering words: 0.9987714987714987
